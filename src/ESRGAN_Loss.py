import lpips
import torch
import torch.nn.functional
import torchvision

__all__ = [
    "LPIPSLoss", "TVLoss", "VGGLoss"
]


class LPIPSLoss(torch.nn.Module):
    r"""The loss value between two images is calculated based on LPIPS.
    `"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric" <https://arxiv.org/pdf/1801.03924.pdf>`_
    Compared with most VGg based loss, it can't achieve good visual effect at large resolution,
    at least in human visual system. So we adopt a perceptual loss based approach.
    """

    def __init__(self, net="vgg") -> None:
        super(LPIPSLoss, self).__init__()
        self.criterion = lpips.LPIPS(net=net).eval()

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        lpips_loss = self.criterion(input, target)

        return lpips_loss


# Source from `https://github.com/jxgu1016/Total_Variation_Loss.pytorch/blob/master/TVLoss.py`
class TVLoss(torch.nn.Module):
    r"""Regularization loss based on Li FeiFei."""

    def __init__(self, weight: torch.Tensor) -> None:
        """The weight information of loss is based on the image information generated by the generator.
        Args:
            weight (tensor): Fake high resolution image weight.
        """
        super(TVLoss, self).__init__()
        self.weight = weight

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        batch_size = input.size()[0]
        h_x = input.size()[2]
        w_x = input.size()[3]
        count_h = self.tensor_size(input[:, :, 1:, :])
        count_w = self.tensor_size(input[:, :, :, 1:])
        h_tv = torch.pow((input[:, :, 1:, :] - input[:, :, :h_x - 1, :]), 2).sum()
        w_tv = torch.pow((input[:, :, :, 1:] - input[:, :, :, :w_x - 1]), 2).sum()
        tv_loss = self.weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size

        return tv_loss

    @staticmethod
    def tensor_size(t):
        return t.size()[1] * t.size()[2] * t.size()[3]


class VGGLoss(torch.nn.Module):
    r""" Where VGG19 represents the feature map of 7/8/35/36th layer in pretrained VGG19 model.
    `"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network" <https://arxiv.org/pdf/1609.04802.pdf>`_
    `"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks" <https://arxiv.org/pdf/1809.00219.pdf>`_
    `"Perceptual Extreme Super Resolution Network with Receptive Field Block" <https://arxiv.org/pdf/2005.12597.pdf>`_
    A loss defined on feature maps of higher level features from deeper network layers
    with more potential to focus on the content of the images. We refer to this network
    as ESRGAN in the following.
    """

    def __init__(self, feature_layer: int = 35) -> None:

        super(VGGLoss, self).__init__()
        model = torchvision.models.vgg19(pretrained=True)
        self.features = torch.nn.Sequential(*list(model.features.children())[:feature_layer]).eval()
        # Freeze parameters. Don't train.
        for name, param in self.features.named_parameters():
            param.requires_grad = False

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        vgg_loss = torch.nn.functional.l1_loss(self.features(input), self.features(target))

        return vgg_loss